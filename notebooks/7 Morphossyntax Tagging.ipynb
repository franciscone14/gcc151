{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphosyntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download corpora:\n",
    "- Macmorpho\n",
    "- Floresta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floresta: corpus sintatico, Mac_Morpho: morfosintatico\n",
    "from nltk.corpus import mac_morpho, floresta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mac_morpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51397\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = []\n",
    "\n",
    "for sentence in mac_morpho.tagged_sents():\n",
    "    tagged_sentences.append(sentence)\n",
    "print(len(tagged_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floresta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9266\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = []\n",
    "def simplify_tag_floresta(t):\n",
    "    if \"+\" in t:\n",
    "        return t[t.index(\"+\")+1:].upper()\n",
    "    else:\n",
    "        return t.upper()\n",
    "\n",
    "for sentence in floresta.tagged_sents():\n",
    "    sentence = [(text, simplify_tag_floresta(tag)) for text, tag in sentence]\n",
    "    tagged_sentences.append(sentence)\n",
    "print(len(tagged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'!': 919,\n",
       "         '\"': 21069,\n",
       "         '$': 60,\n",
       "         \"'\": 510,\n",
       "         '(': 7713,\n",
       "         '((': 10,\n",
       "         ')': 7741,\n",
       "         '))': 10,\n",
       "         ',': 68494,\n",
       "         '-': 3117,\n",
       "         '.': 82,\n",
       "         '...': 84,\n",
       "         '/': 95,\n",
       "         ':': 6736,\n",
       "         ';': 1376,\n",
       "         '=': 14,\n",
       "         '?': 1510,\n",
       "         'ADJ': 53372,\n",
       "         'ADJ|+': 11,\n",
       "         'ADJ|EST': 174,\n",
       "         'ADV': 30653,\n",
       "         'ADV-KS': 392,\n",
       "         'ADV-KS-REL': 898,\n",
       "         'ADV|+': 8,\n",
       "         'ADV|EST': 9,\n",
       "         'ADV|HOR': 9,\n",
       "         'ADV|[': 18,\n",
       "         'ADV|]': 13,\n",
       "         'ART': 151891,\n",
       "         'ART|+': 7,\n",
       "         'ART|EST': 4,\n",
       "         'CUR': 2706,\n",
       "         'IN': 415,\n",
       "         'IN|EST': 1,\n",
       "         'KC': 28262,\n",
       "         'KC|+': 2,\n",
       "         'KC|EST': 2,\n",
       "         'KC|[': 143,\n",
       "         'KC|]': 133,\n",
       "         'KS': 14320,\n",
       "         'KS|[': 5,\n",
       "         'KS|]': 4,\n",
       "         'N': 236462,\n",
       "         'NPRO': 3,\n",
       "         'NPROP': 114318,\n",
       "         'NPROP|+': 35,\n",
       "         'NUM': 18110,\n",
       "         'NUM|TEL': 2,\n",
       "         'N|AP': 4350,\n",
       "         'N|DAD': 255,\n",
       "         'N|DAT': 285,\n",
       "         'N|EST': 2807,\n",
       "         'N|HOR': 913,\n",
       "         'N|TEL': 866,\n",
       "         'PCP': 23092,\n",
       "         'PDEN': 6786,\n",
       "         'PDEN|EST': 2,\n",
       "         'PREP': 104364,\n",
       "         'PREP|': 2,\n",
       "         'PREP|+': 78274,\n",
       "         'PREP|+]': 7,\n",
       "         'PREP|EST': 3,\n",
       "         'PREP|[': 20,\n",
       "         'PREP|]': 9,\n",
       "         'PRO-KS': 2150,\n",
       "         'PRO-KS-REL': 11347,\n",
       "         'PROADJ': 20919,\n",
       "         'PROADJ|+': 2,\n",
       "         'PROP': 7,\n",
       "         'PROPESS': 14718,\n",
       "         'PROPESS|+': 2,\n",
       "         'PROPESS|EST': 2,\n",
       "         'PROSUB': 8598,\n",
       "         'V': 98056,\n",
       "         'VAUX': 17832,\n",
       "         'VAUX|!': 1,\n",
       "         'VAUX|+': 99,\n",
       "         'V|!': 1,\n",
       "         'V|+': 2373,\n",
       "         'V|EST': 5,\n",
       "         '[': 23,\n",
       "         '`': 3})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tags = []\n",
    "for sent in tagged_sentences:\n",
    "    for w, t in sent:\n",
    "        all_tags.append(t)\n",
    "\n",
    "tagset_freq = Counter(all_tags)\n",
    "tagset_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pode ser feito um tratamento para reduzir o número de classes e melhorar a acuracia\n",
    "- Metricas de avaliação:\n",
    "    - Precisão: do que eu classifiquei quanto eu acertei\n",
    "    - Cobertura: de todo o conjunto quanto eu classifiquei correto\n",
    "    - F-Measure: Media armonica das duas anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41117\n",
      "10280\n"
     ]
    }
   ],
   "source": [
    "# Embaralhar pode melhorar principalmente se o floresta e o mac_morfo forem concatenados\n",
    "\n",
    "cutoff = int(.80 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    " \n",
    "print(len(training_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19676506865571755"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifica todo mundo como N => Dummy classifier => Classe majoritaria\n",
    "tagger_default = nltk.DefaultTagger('N')\n",
    "tagger_default.evaluate(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=43530, backoff=25.59%, pruning=23.91%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8156664993560486"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recebe a etiqueta mais comum que apresenta a palavras\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(training_sentences, verbose=True, backoff=tagger_default)\n",
    "unigram_tagger.evaluate(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=12515, backoff=56.22%, pruning=90.08%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8298100829938484"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Olha a janela de palavras antes e depois: no caso 2 palavras\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(training_sentences, verbose=True, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=12937, backoff=66.56%, pruning=94.25%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8297011365449191"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Janela de 3 palavras\n",
    "\n",
    "trigram_tagger = nltk.TrigramTagger(training_sentences, verbose=True, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default\n",
      "[('Uma', 'N'), ('vez', 'N'), ('que', 'N'), ('você', 'N'), ('tenha', 'N'), ('aceitado', 'N'), ('suas', 'N'), ('falhas,', 'N'), ('ninguém', 'N'), ('poderá', 'N'), ('usá-las', 'N'), ('contra', 'N'), ('você', 'N')]\n",
      "Unigram\n",
      "[('Uma', 'ART'), ('vez', 'N'), ('que', 'PRO-KS-REL'), ('você', 'PROPESS'), ('tenha', 'VAUX'), ('aceitado', 'N'), ('suas', 'PROADJ'), ('falhas,', 'N'), ('ninguém', 'PROSUB'), ('poderá', 'VAUX'), ('usá-las', 'N'), ('contra', 'PREP'), ('você', 'PROPESS')]\n",
      "Bigram\n",
      "[('Uma', 'ART'), ('vez', 'N'), ('que', 'PRO-KS-REL'), ('você', 'PROPESS'), ('tenha', 'VAUX'), ('aceitado', 'N'), ('suas', 'PROADJ'), ('falhas,', 'N'), ('ninguém', 'PROSUB'), ('poderá', 'VAUX'), ('usá-las', 'N'), ('contra', 'PREP'), ('você', 'PROPESS')]\n",
      "Trigram\n",
      "[('Uma', 'ART'), ('vez', 'N'), ('que', 'PRO-KS-REL'), ('você', 'PROPESS'), ('tenha', 'VAUX'), ('aceitado', 'N'), ('suas', 'PROADJ'), ('falhas,', 'N'), ('ninguém', 'PROSUB'), ('poderá', 'VAUX'), ('usá-las', 'N'), ('contra', 'PREP'), ('você', 'PROPESS')]\n"
     ]
    }
   ],
   "source": [
    "# Do = de + o ele classifica certo, aconcelhavel fazer isso como proprocessamento\n",
    "\n",
    "sentence = \"Uma vez que você tenha aceitado suas falhas, ninguém poderá usá-las contra você\".split()\n",
    "print(\"Default\")\n",
    "tagged_sentence = tagger_default.tag(sentence)\n",
    "print(tagged_sentence)\n",
    "print(\"Unigram\")\n",
    "tagged_sentence = unigram_tagger.tag(sentence)\n",
    "print(tagged_sentence)\n",
    "print(\"Bigram\")\n",
    "tagged_sentence = bigram_tagger.tag(sentence)\n",
    "print(tagged_sentence)\n",
    "print(\"Trigram\")\n",
    "tagged_sentence = trigram_tagger.tag(sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o objeto treinado\n",
    "import pickle\n",
    "\n",
    "with open('bigram_tagger.pickle', 'wb') as p_file:\n",
    "    p_file.write(pickle.dumps(bigram_tagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other corpus\n",
    "http://www.nilc.icmc.usp.br/nilc/download/corpus100.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "command = \"python -m spacy download pt_core_news_sm\".split()\n",
    "subprocess.call(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-37bb58fdca65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pt_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"O rato comeu a roupa do rei de roma\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "sentence = \"O rato comeu a roupa do rei de roma\"\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model from http://nilc.icmc.usp.br/nlpnet/data/pos-pt.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpnet.set_data_dir('pos-pt/')\n",
    "sentence = \"O rato comeu a roupa do rei de roma\"\n",
    "tagger = nlpnet.POSTagger()\n",
    "tagger.tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
