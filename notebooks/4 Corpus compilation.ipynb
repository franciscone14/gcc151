{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus compilation\n",
    "- text type and genres\n",
    "- characteristics according to task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests in python\n",
    "import urllib3\n",
    "# Regular expression library\n",
    "import re\n",
    "# Transform html into a tree im memory\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "# Get just the text in a html document\n",
    "import justext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Disable https warning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Pass as a browser \n",
    "user_agent = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "\n",
    "# Pool of boots to make requests\n",
    "http = urllib3.PoolManager(10, headers=user_agent)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self, corpus_path, max_files, seed_url, url_pattern):\n",
    "        self.corpus_path = corpus_path # corpus address\n",
    "        self.max_files = max_files # max amount of sizes\n",
    "        self.seed_url = seed_url # root url\n",
    "        self.url_pattern = url_pattern # Select links of interest\n",
    "        self.visited_links = {} # Hash to store viseted links\n",
    "        self.to_be_visited = [] # List of link\n",
    "        \n",
    "        # If path not exists create it\n",
    "        if not os.path.exists(self.corpus_path):\n",
    "            os.makedirs(self.corpus_path)\n",
    "        \n",
    "    def crawl(self):\n",
    "        first_urls = self.get_page(self.seed_url)\n",
    "        self.add_links(first_urls)\n",
    "        next_link = self.get_next_link()\n",
    "        \n",
    "        file_counter = 1\n",
    "        while next_link and file_counter < self.max_files:\n",
    "            links = self.get_page(next_link)\n",
    "            self.add_links(links)\n",
    "            next_link = self.get_next_link()\n",
    "            file_counter += 1\n",
    "        \n",
    "    \n",
    "    def get_page(self, url):\n",
    "        print(\"getting page {}\".format(url))\n",
    "        response = http.request('GET', url)\n",
    "\n",
    "        # store text content\n",
    "        paragraphs = justext.justext(response.data, justext.get_stoplist(\"Portuguese\"))\n",
    "        with open(\"{}/{}.txt\".format(self.corpus_path, url.replace(\".\", \"_\").replace(\"/\",\"-\")), \"w\") as output_file:\n",
    "            for paragraph in paragraphs:\n",
    "                # Boilerplate is everthing that is not the main text\n",
    "                if not paragraph.is_boilerplate:\n",
    "                    output_file.write(paragraph.text)\n",
    "        \n",
    "        # get links\n",
    "        soup = BeautifulSoup(response.data, 'html.parser')\n",
    "        links_1 = re.search(self.url_pattern, str(response.data))\n",
    "        print(links_1)\n",
    "        links = []\n",
    "        for group in links_1.groups():\n",
    "            links.append(\"https://linuxdicasesuporte.blogspot.com/2019/{}\".format(group))\n",
    "            \n",
    "        \n",
    "#         links = [link.get('href') for link in soup.findAll('meta', attrs={'content': re.compile(self.url_pattern)})]\n",
    "        print(links)\n",
    "        return links\n",
    "\n",
    "    def add_links(self, links):\n",
    "        links = list(set(links))\n",
    "        self.to_be_visited.extend([link for link in links if link not in self.visited_links])\n",
    "\n",
    "    def get_next_link(self):\n",
    "        next_link = self.to_be_visited.pop(0)\n",
    "        self.visited_links[next_link] = None\n",
    "        return next_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crawler_seguranca = Crawler(\"data/corpora/seguranca\", 50, \n",
    "                             \"https://linuxdicasesuporte.blogspot.com/2019/04/debian-11-bullseye.html\", \n",
    "                            '\"https://linuxdicasesuporte\\.blogspot\\.com/(.*?)\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting page https://linuxdicasesuporte.blogspot.com/2019/04/debian-11-bullseye.html\n",
      "<re.Match object; span=(864, 925), match='\"https://linuxdicasesuporte.blogspot.com/feeds/po>\n",
      "['https://linuxdicasesuporte.blogspot.com/2019/feeds/posts/default']\n",
      "getting page https://linuxdicasesuporte.blogspot.com/2019/feeds/posts/default\n",
      "<re.Match object; span=(858, 919), match='\"https://linuxdicasesuporte.blogspot.com/feeds/po>\n",
      "['https://linuxdicasesuporte.blogspot.com/2019/feeds/posts/default']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-d8bae1e77d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawler_seguranca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-6d50a81c3ec2>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mnext_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mfile_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-6d50a81c3ec2>\u001b[0m in \u001b[0;36mget_next_link\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mnext_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_be_visited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisited_links\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_link\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "crawler_seguranca.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
