{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franciscone/.envs/nlp/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from franc_lib.semantics import Semantics\n",
    "from franc_lib.lexical import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Semantics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning....\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franciscone/.envs/nlp/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/franciscone/.envs/nlp/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/franciscone/.envs/nlp/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished !\n",
      "Precisão 55.5204\n",
      "[[  20   16    3   12   60   38]\n",
      " [   1  105   46   66  125  123]\n",
      " [   0   47   58  128  240   79]\n",
      " [   4   27   25  281 1191  271]\n",
      " [   0    9    5  151 3219 1656]\n",
      " [   2    5    4   25 1547 3689]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sentiment_analysis(text=\"Essa geladeira é ruim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v/v\t 0.0\t 1.0\t 2.0\t 3.0\t 4.0\t 5.0\n",
      "0.0\t 20\t 16\t 3\t 12\t 60\t 38\t\n",
      "1.0\t 1\t 105\t 46\t 66\t 125\t 123\t\n",
      "2.0\t 0\t 47\t 58\t 128\t 240\t 79\t\n",
      "3.0\t 4\t 27\t 25\t 281\t 1191\t 271\t\n",
      "4.0\t 0\t 9\t 5\t 151\t 3219\t 1656\t\n",
      "5.0\t 2\t 5\t 4\t 25\t 1547\t 3689\t\n"
     ]
    }
   ],
   "source": [
    "s.get_confusion_matrix()\n",
    "# s.classifier.predict(s.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b551b169dc4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_classes' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_classes, s.classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "from xml.parsers.expat import ExpatError\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    return Translator().translate(text, dest='pt')\n",
    "\n",
    "norm = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate(\"Robert DeNiro plays the most unbelievably intelligent illiterate of all time. This movie is so wasteful of talent, it is truly disgusting. The script is unbelievable. The dialog is unbelievable. Jane Fonda's character is a caricature of herself, and not a funny one. The movie moves at a snail's pace, is photographed in an ill-advised manner, and is insufferably preachy. It also plugs in every cliche in the book. Swoozie Kurtz is excellent in a supporting role, but so what?<br /><br />Equally annoying is this new IMDB rule of requiring ten lines for every review. When a movie is this worthless, it doesn't require ten lines of text to let other readers know that it is a waste of time and tape. Avoid this movie.\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'aclImdb/train/'\n",
    "\n",
    "dataset_2 = {'polarity': [], 'bin_polarity':[], 'review':[], 'set':[]}\n",
    "\n",
    "probability = 0.8\n",
    "\n",
    "for folder in os.listdir(corpus_path):\n",
    "    i = 0\n",
    "    if \".\" not in folder and \"unsup\" not in folder:\n",
    "        for file in os.listdir('{}{}/'.format(corpus_path, folder)):\n",
    "            score = int(file.split('.')[0].split('_')[1])\n",
    "            with open('{}{}/{}'.format(corpus_path, folder, file), 'r') as txt_file:\n",
    "                comment = \"\"\n",
    "                for line in txt_file.readlines():\n",
    "                    comment += translate(line).text + \" \"\n",
    "#                     print(comment)\n",
    "#                     import time\n",
    "#                     time.sleep(5)\n",
    "                dataset_2['polarity'].append(score)\n",
    "                dataset_2['bin_polarity'].append(0 if score <= 5 else 1)\n",
    "                dataset_2['review'].append(comment)\n",
    "                dataset_2['set'].append('train' if np.random.rand() < probability else 'test')\n",
    "            i += 1\n",
    "            print(i)\n",
    "            if i == 2000: break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2[\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = 'trainset/'\n",
    "\n",
    "dataset = {'polarity':[], 'bin_polarity': [], 'review':[], 'set':[]}\n",
    "\n",
    "probability = 0.8\n",
    "\n",
    "for product in os.listdir(path):\n",
    "    for score in os.listdir(path + product):\n",
    "        for file in os.listdir(path + product + \"/\" + score + \"/\"):\n",
    "            if file.endswith('.xml'):\n",
    "                with open(path + product + \"/\" + score + \"/\" + file, 'r') as xml:\n",
    "                    try:\n",
    "                        data = xml.read()\n",
    "                        data_dict = xmltodict.parse(data)\n",
    "                        \n",
    "                        if float(score) < 4:\n",
    "#                             print(\"CAIU\")\n",
    "                            dataset['polarity'].append(float(data_dict['review']['stars']['@value']))\n",
    "                            dataset['bin_polarity'].append(0 if float(data_dict['review']['stars']['@value']) < 3.0 else 1)\n",
    "                            dataset['review'].append(data_dict['review']['cons'].replace('\\n',''))\n",
    "                            dataset['set'].append('train' if np.random.rand() < probability else 'test')\n",
    "                        \n",
    "                        # Comments\n",
    "                        dataset['polarity'].append(float(data_dict['review']['stars']['@value']))\n",
    "                        dataset['bin_polarity'].append(0 if float(data_dict['review']['stars']['@value']) < 3.0 else 1)\n",
    "                        dataset['review'].append(data_dict['review']['opinion'].replace('\\n',''))\n",
    "                        dataset['set'].append('train' if np.random.rand() < probability else 'test')\n",
    "                    except:\n",
    "                        dataset['polarity'].pop(0)\n",
    "                        dataset['bin_polarity'].pop(0)\n",
    "                        \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['review'] = dataset['review'][:-5]\n",
    "# dataset['set'] = dataset['set'][:-5]\n",
    "\n",
    "print(len(dataset['polarity']))\n",
    "print(len(dataset['bin_polarity']))\n",
    "print(len(dataset['review']))\n",
    "print(len(dataset['set']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "dataframe = pd.DataFrame(data=dataset)\n",
    "dataframe.groupby('polarity').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words = []\n",
    "\n",
    "with open('LIWC2007_Portugues_win.dic.txt', 'r', encoding='latin') as liwc_file:\n",
    "    in_header = True\n",
    "    for line in liwc_file.readlines():\n",
    "        if not re.match('^\\d+', line):\n",
    "            parts = line.split()\n",
    "            word = parts.pop(0)\n",
    "            if '126' in parts or '127' in parts:\n",
    "                words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from franc_lib.lexical import Preprocessing\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('pt_core_news_sm')\n",
    "stopwords = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "# print(stopwords)\n",
    "\n",
    "cont = 0\n",
    "for word in words:\n",
    "    if word in stopwords:\n",
    "        stopwords.remove(word)\n",
    "\n",
    "normalizer = Preprocessing()\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = normalizer.lowercase(text)\n",
    "    text = normalizer.remove_punctuation(text)\n",
    "    tokens = normalizer.tokenize_words(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['normalized_review'] = dataframe['review'].apply(preprocessing)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = dataframe[dataframe['set'] == 'train']['normalized_review'].values.tolist()\n",
    "train_classes = dataframe[dataframe['set'] == 'train']['polarity'].values.tolist()\n",
    "test_reviews = dataframe[dataframe['set'] == 'test']['normalized_review'].values.tolist()\n",
    "test_classes = dataframe[dataframe['set'] == 'test']['polarity'].values.tolist()\n",
    "\n",
    "transformer = TfidfVectorizer()\n",
    "transformer.fit(train_reviews)\n",
    "X = transformer.transform(train_reviews)\n",
    "X_test = transformer.transform(test_reviews)\n",
    "\n",
    "# cv = CountVectorizer(binary=True)\n",
    "# cv.fit(train_reviews)\n",
    "# X = cv.transform(train_reviews)\n",
    "# X_test = cv.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(verbose=True)\n",
    "classifier.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svr = SVR()\n",
    "# svr.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=(50, 5), verbose=True)\n",
    "# clf = MLPClassifier(solver='sgd'), n_jobs=4))\n",
    "\n",
    "mlp.learning_rate_initial = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, mlp.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=3, criterion=\"entropy\", verbose=1)\n",
    "rf.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= \"\"\n",
    "print(preprocessing(sentence))\n",
    "X = transformer.transform([preprocessing(sentence)])\n",
    "rf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(15, weights='distance')\n",
    "knn.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, knn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
