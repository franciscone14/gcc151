{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "Read https://web.stanford.edu/~jurafsky/slp3/19.pdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tarefa: Olha nao somente os rotulos do NIW, e tratar os casos do *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/franciscone/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/franciscone/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/franciscone/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', 'punkt', 'rslp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from franc_lib.lexical import Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stopwords = stopwords.words('portuguese')\n",
    "normalizer = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download LIWC resource at http://143.107.183.175:21380/portlex/images/arquivos/liwc/LIWC2007_Portugues_win.dic.txt\n",
    "# posemo = 126\n",
    "# negemo = 127\n",
    "# what more?\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "with open('LIWC2007_Portugues_win.dic.txt', 'r', encoding='latin') as liwc_file:\n",
    "    in_header = True\n",
    "    for line in liwc_file.readlines():\n",
    "        if not re.match('^\\d+', line):\n",
    "            parts = line.split()\n",
    "            word = parts.pop(0)\n",
    "            if '126' in parts:\n",
    "                positives.append(word)\n",
    "            elif '127' in parts:\n",
    "                negatives.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'não' in positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'nao' in negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sentment_analysis(text, binary=False):\n",
    "    text = normalizer.remove_punctuation(text)\n",
    "    print(text)\n",
    "    tokens = normalizer.tokenize_words(text)\n",
    "    tokens = normalizer.remove_stopwords(tokens)\n",
    "    \n",
    "    polarity = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        print(token)\n",
    "        if token in positives:\n",
    "            polarity += 1\n",
    "        elif token in negatives:\n",
    "            polarity -= 1\n",
    "    if not binary:\n",
    "        return polarity\n",
    "    else:\n",
    "        if polarity < 0:\n",
    "            polarity = -1\n",
    "        elif polarity > 0:\n",
    "            polarity = 1\n",
    "        \n",
    "        return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'det', internet), ('internet', 'nsubj', boa), ('não', 'advmod', boa), ('está', 'cop', boa), ('muito', 'advmod', boa), ('boa', 'ROOT', boa)]\n"
     ]
    }
   ],
   "source": [
    "from franc_lib.syntax import Preprocessing\n",
    "\n",
    "import subprocess\n",
    "command = \"python -m spacy download pt_core_news_sm\".split()\n",
    "subprocess.call(command)\n",
    "\n",
    "p = Preprocessing(\"pt_core_news_sm\")\n",
    "aux = p.parse(\"A internet não está muito boa\");\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A internet não está muito boa\n",
      "A\n",
      "internet\n",
      "está\n",
      "boa\n",
      "Positiva\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_sentment_analysis(text='A internet não está muito boa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_polarity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>trainset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Clever, gritty, witty, fast-paced, sexy, extra...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Perhaps I would have liked this film more if I...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Thank you Mario Van Peebles for informing us o...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Hello Playmates.I recently watched this film f...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Put simply, this mini-series was terrible. Let...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_polarity polarity                                             review  \\\n",
       "0             1       10  Clever, gritty, witty, fast-paced, sexy, extra...   \n",
       "1             1        7  Perhaps I would have liked this film more if I...   \n",
       "2             1        9  Thank you Mario Van Peebles for informing us o...   \n",
       "3             1        9  Hello Playmates.I recently watched this film f...   \n",
       "4             0        1  Put simply, this mini-series was terrible. Let...   \n",
       "\n",
       "  trainset  \n",
       "0    train  \n",
       "1    train  \n",
       "2     test  \n",
       "3    train  \n",
       "4     test  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using dataset of IMDb, available at: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "import wget\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"dataset/aclImdb_v1.tar.gz\"\n",
    "\n",
    "\n",
    "# Donwload data\n",
    "\n",
    "dataset_link = \"http://ai.stanford.edu/~amaas/data/sentiment/{}\".format(\"aclImdb_v1.tar.gz\")\n",
    "try:\n",
    "    os.mkdir(\"dataset\")\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    file = wget.download(dataset_link, out='dataset/aclImdb_v1.tar.gz')\n",
    "    tar = tarfile.open(filename, \"r:gz\")\n",
    "    tar.extractall(\"dataset\")\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "# read data\n",
    "\n",
    "dataset_path = 'dataset/aclImdb'\n",
    "train_positive_files = ['train/pos/'+f for f in os.listdir(dataset_path+'/train/pos') \\\n",
    "                        if os.path.isfile(os.path.join(dataset_path+'/train/pos', f))]\n",
    "\n",
    "train_negative_files = ['train/neg/'+f for f in os.listdir(dataset_path+'/train/neg') \\\n",
    "                        if os.path.isfile(os.path.join(dataset_path+'/train/neg', f))]\n",
    "\n",
    "test_positive_files = ['test/pos/'+f for f in os.listdir(dataset_path+'/test/pos') \\\n",
    "                       if os.path.isfile(os.path.join(dataset_path+'/test/pos', f))]\n",
    "\n",
    "test_negative_files = ['test/neg/'+f for f in os.listdir(dataset_path+'/test/neg') \\\n",
    "                       if os.path.isfile(os.path.join(dataset_path+'/test/neg', f))]\n",
    "\n",
    "all_files = list(set().union(train_positive_files,train_negative_files, test_positive_files, test_negative_files))\n",
    "\n",
    "dataset = {'trainset':[], 'polarity':[], 'bin_polarity': [], 'review':[]}\n",
    "\n",
    "for file in all_files:\n",
    "    polarity = file.split('.')[0].split('_')[1]\n",
    "    with open(os.path.join(dataset_path, file), 'r') as text_file:\n",
    "        dataset['trainset'].append(file.split('/')[0])\n",
    "        bin_polarity = 1 if int(polarity) > 5 else 0  # transform into binary polarity\n",
    "        dataset['bin_polarity'].append(bin_polarity)\n",
    "        dataset['polarity'].append(polarity)\n",
    "        dataset['review'].append(text_file.readlines()[0])\n",
    "\n",
    "        \n",
    "# create dataframe\n",
    "\n",
    "dataframe = pd.DataFrame(data=dataset)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_polarity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>trainset</th>\n",
       "      <th>normalized_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Clever, gritty, witty, fast-paced, sexy, extra...</td>\n",
       "      <td>train</td>\n",
       "      <td>clever gritty witty fastpaced sexy extravagant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Perhaps I would have liked this film more if I...</td>\n",
       "      <td>train</td>\n",
       "      <td>perhaps would liked film wasnt attached charac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Thank you Mario Van Peebles for informing us o...</td>\n",
       "      <td>test</td>\n",
       "      <td>thank mario van peebles informing us existence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Hello Playmates.I recently watched this film f...</td>\n",
       "      <td>train</td>\n",
       "      <td>hello playmatesi recently watched film first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Put simply, this mini-series was terrible. Let...</td>\n",
       "      <td>test</td>\n",
       "      <td>put simply miniseries terrible let count ways ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_polarity polarity                                             review  \\\n",
       "0             1       10  Clever, gritty, witty, fast-paced, sexy, extra...   \n",
       "1             1        7  Perhaps I would have liked this film more if I...   \n",
       "2             1        9  Thank you Mario Van Peebles for informing us o...   \n",
       "3             1        9  Hello Playmates.I recently watched this film f...   \n",
       "4             0        1  Put simply, this mini-series was terrible. Let...   \n",
       "\n",
       "  trainset                                  normalized_review  \n",
       "0    train  clever gritty witty fastpaced sexy extravagant...  \n",
       "1    train  perhaps would liked film wasnt attached charac...  \n",
       "2     test  thank mario van peebles informing us existence...  \n",
       "3    train  hello playmatesi recently watched film first t...  \n",
       "4     test  put simply miniseries terrible let count ways ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = normalizer.lowercase(text)\n",
    "    text = normalizer.remove_punctuation(text)\n",
    "    tokens = normalizer.tokenize_words(text)\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "dataframe['normalized_review'] = dataframe['review'].apply(preprocessing)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Trainer o WordEmbeding para fornece como entrada para o classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-42a73aae79ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin_polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin_polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "train_reviews = dataframe[dataframe['trainset'] == 'train']['normalized_review'].values.tolist()\n",
    "train_classes = dataframe[dataframe['trainset'] == 'train']['bin_polarity'].values.tolist()\n",
    "test_reviews = dataframe[dataframe['trainset'] == 'test']['normalized_review'].values.tolist()\n",
    "test_classes = dataframe[dataframe['trainset'] == 'test']['bin_polarity'].values.tolist()\n",
    "\n",
    "transformer = TfidfVectorizer()\n",
    "transformer.fit(train_reviews)\n",
    "X = transformer.transform(train_reviews)\n",
    "X_test = transformer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/GCC151_env/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC()\n",
    "classifier.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65392"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_classes, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erickmaziero/virtualenvs/GCC151_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_lr = LogisticRegression()\n",
    "classifier_lr.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88452"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_classes, classifier_lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film really bad\n",
      "  (0, 87080)\t0.5961343855298462\n",
      "  (0, 39451)\t0.43855537677700446\n",
      "  (0, 9716)\t0.6725273049393103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This film was really bad!\"\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "print(preprocessed_sentence)\n",
    "instance = transformer.transform([preprocessing(sentence)])\n",
    "print(instance)\n",
    "classifier.predict(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good film\n",
      "  (0, 45171)\t0.7750865021715609\n",
      "  (0, 39451)\t0.6318551369985488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Good film!\"\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "print(preprocessed_sentence)\n",
    "instance = transformer.transform([preprocessing(sentence)])\n",
    "print(instance)\n",
    "classifier.predict(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
